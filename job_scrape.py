import asyncio
from playwright.async_api import async_playwright
from bs4 import BeautifulSoup
from datetime import datetime
import os

target_team = "Gå¤§é˜ª"
url = "https://soccer.yahoo.co.jp/jleague/category/j1/teams/128/schedule?gk=2"

async def scrape_gamba_schedule(url):
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        page = await browser.new_page()
        print("ğŸŒ ã‚¢ã‚¯ã‚»ã‚¹é–‹å§‹...")
        await page.goto(url, wait_until='networkidle')
        await page.wait_for_selector("section#scheduleTable table.sc-tableGame")

        os.makedirs("cache", exist_ok=True)
        await page.screenshot(path="cache/screenshot.png", full_page=True)
        html = await page.content()
        await browser.close()

        with open("cache/debug_team_page.html", "w", encoding="utf-8") as f:
            f.write(html)
        print("âœ… HTMLä¿å­˜æ¸ˆã¿")

        soup = BeautifulSoup(html, 'html.parser')
        rows = soup.select("table.sc-tableGame tbody tr")
        print(f"âœ… è©¦åˆè¡Œæ•°: {len(rows)}")

        match_info = []
        for i, row in enumerate(rows):
            try:
                date_elem = row.select_one("td.sc-tableGame__data--date")
                teams_elem = row.select("td.sc-tableGame__data--team")
                stadium_elem = row.select_one("td.sc-tableGame__data--venue")

                # ãƒ­ã‚°å‡ºåŠ›ï¼ˆãƒ‡ãƒãƒƒã‚°ï¼‰
                print(f"\n[DEBUG] Match Row {i+1}")
                print(f"  date_elem: {date_elem}")
                print(f"  teams_elem: {teams_elem}")
                print(f"  stadium_elem: {stadium_elem}")

                if not date_elem or len(teams_elem) < 2:
                    print("  â›” ãƒ‡ãƒ¼ã‚¿ä¸è¶³ã§ã‚¹ã‚­ãƒƒãƒ—")
                    continue

                date_text = date_elem.get_text(strip=True).split()[0]
                try:
                    match_date = datetime.strptime(date_text, "%m/%d")
                    match_date = match_date.replace(year=datetime.now().year)
                except:
                    print("  â›” æ—¥ä»˜ãƒ‘ãƒ¼ã‚¹å¤±æ•—")
                    continue

                if match_date.date() < datetime.today().date():
                    print("  â­ éå»ã®è©¦åˆãªã®ã§ã‚¹ã‚­ãƒƒãƒ—")
                    continue

                home_team = teams_elem[0].get_text(strip=True)
                away_team = teams_elem[1].get_text(strip=True)

                print(f"  ãƒãƒ¼ãƒ : {home_team} vs {away_team}")
                if target_team not in [home_team, away_team]:
                    print("  â­ Gå¤§é˜ªã§ã¯ãªã„ãŸã‚ã‚¹ã‚­ãƒƒãƒ—")
                    continue

                opponent = away_team if home_team == target_team else home_team
                stadium = stadium_elem.get_text(strip=True) if stadium_elem else "ã‚¹ã‚¿ã‚¸ã‚¢ãƒ ä¸æ˜"

                match_info.append({
                    "date": match_date.strftime("%Y/%m/%d"),
                    "teams": f"{target_team} vs {opponent}",
                    "stadium": stadium
                })
                print("  âœ… è©¦åˆæƒ…å ±è¿½åŠ æ¸ˆã¿")

            except Exception as e:
                print(f"  âŒ ä¾‹å¤–ç™ºç”Ÿ: {e}")

        return match_info

def save_to_cache(info):
    os.makedirs("cache", exist_ok=True)
    with open("cache/match_info.txt", "w", encoding="utf-8") as f:
        if info:
            next_match = info[0]
            f.write(f"{target_team}ã®æ¬¡ã®è©¦åˆ: {next_match['date']} {next_match['teams']} @ {next_match['stadium']}")
        else:
            f.write("è©¦åˆæƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸâ€¦")
    print("âœ… match_info.txt ã«ä¿å­˜å®Œäº†")

if __name__ == "__main__":
    match_info = asyncio.run(scrape_gamba_schedule(url))
    save_to_cache(match_info)
    print("ğŸ‰ å®Œäº†ï¼")
